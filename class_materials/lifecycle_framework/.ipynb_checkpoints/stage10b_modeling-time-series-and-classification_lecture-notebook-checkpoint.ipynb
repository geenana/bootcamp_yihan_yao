{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f23fd4e2",
   "metadata": {},
   "source": [
    "# Modeling: Time Series & Classification — Live Lecture Notebook\n",
    "*Session: Aug 22 (Fri)*\n",
    "\n",
    "This notebook generates its **own synthetic data** (no external files) and demonstrates:\n",
    "- Lag & rolling features for time series\n",
    "- Time-aware train/test split vs random split\n",
    "- A baseline return-forecast model\n",
    "- A binary classifier for next-day direction\n",
    "- Proper evaluation (RMSE / precision, recall, F1) and pitfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e49fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Setup\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set()\n",
    "plt.rcParams['figure.figsize'] = (9, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be955b13",
   "metadata": {},
   "source": [
    "## 1) Generate Synthetic Financial Series\n",
    "We simulate a **regime-switching geometric random walk** with occasional jumps to create heavy tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bde3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 600  # ~2.5 years of business days\n",
    "dates = pd.bdate_range('2020-01-01', periods=n)\n",
    "\n",
    "# Regime means/vols\n",
    "mu = np.where(np.arange(n) < n//2, 0.0003, -0.0001)\n",
    "sigma = np.where(np.arange(n) < n//2, 0.01, 0.015)\n",
    "\n",
    "eps = np.random.normal(mu, sigma)\n",
    "# Add jumps\n",
    "jumps = np.zeros(n)\n",
    "jump_days = np.random.choice(np.arange(20, n-20), size=6, replace=False)\n",
    "jumps[jump_days] = np.random.normal(0, 0.06, size=len(jump_days))\n",
    "rets = eps + jumps\n",
    "\n",
    "price = 100 * np.exp(np.cumsum(rets))\n",
    "df = pd.DataFrame({'price': price}, index=dates)\n",
    "df['ret'] = df['price'].pct_change().fillna(0.0)\n",
    "df['log_ret'] = np.log1p(df['ret'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df['price'].plot(ax=ax)\n",
    "ax.set_title('Synthetic Price Series'); ax.set_ylabel('Price'); plt.show()\n",
    "fig, ax = plt.subplots()\n",
    "df['ret'].plot(ax=ax)\n",
    "ax.set_title('Daily Returns'); ax.set_ylabel('Return'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f45059c",
   "metadata": {},
   "source": [
    "## 2) Create Time-Series Features\n",
    "- `lag_1`, `lag_5` of returns\n",
    "- `roll_mean_5`\n",
    "- `roll_vol_20` (rolling std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lag_1'] = df['ret'].shift(1)\n",
    "df['lag_5'] = df['ret'].shift(5)\n",
    "df['roll_mean_5'] = df['ret'].rolling(5).mean().shift(1)\n",
    "df['roll_vol_20'] = df['ret'].rolling(20).std().shift(1)\n",
    "\n",
    "# Targets\n",
    "df['y_next_ret'] = df['ret'].shift(-1)\n",
    "df['y_up'] = (df['y_next_ret'] > 0).astype(int)\n",
    "df_feat = df.dropna().copy()\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba856fb7",
   "metadata": {},
   "source": [
    "## 3) Time-Aware Split\n",
    "Train = first 80%, Test = final 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3040ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(len(df_feat)*0.8)\n",
    "train, test = df_feat.iloc[:cut], df_feat.iloc[cut:]\n",
    "features = ['lag_1','lag_5','roll_mean_5','roll_vol_20']\n",
    "X_train, y_train = train[features], train['y_next_ret']\n",
    "X_test,  y_test  = test[features],  test['y_next_ret']\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b709363",
   "metadata": {},
   "source": [
    "## 4) Forecasting Baseline (Regression on Returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Pipeline([('scaler', StandardScaler()), ('linreg', LinearRegression())])\n",
    "reg.fit(X_train, y_train)\n",
    "pred_ts = reg.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, pred_ts)\n",
    "mae = mean_absolute_error(y_test, pred_ts)\n",
    "rmse = mean_squared_error(y_test, pred_ts) ** 0.5  # Compute RMSE manually\n",
    "\n",
    "r2 = r2_score(y_test, pred_ts)\n",
    "mae, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee323923",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "y_test.reset_index(drop=True).plot(ax=ax, label='True')\n",
    "pd.Series(pred_ts).plot(ax=ax, label='Pred', alpha=0.8)\n",
    "ax.set_title('Next-Day Return: True vs Predicted (Test)')\n",
    "ax.legend(); plt.show()\n",
    "\n",
    "resid = y_test.values - pred_ts\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(range(len(resid)), resid, s=10)\n",
    "ax.axhline(0, ls='--')\n",
    "ax.set_title('Residuals (Test)'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a26b90",
   "metadata": {},
   "source": [
    "### TimeSeriesSplit Illustration (no deep CV today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for i, (tr, va) in enumerate(tscv.split(df_feat)):\n",
    "    print(f\"Fold {i+1}: train={tr[:2]}..{tr[-1]}, val={va[:2]}..{va[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dcab9b",
   "metadata": {},
   "source": [
    "## 5) Classification Target: Next-Day Up/Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bcfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc_train, yc_train = train[features], train['y_up']\n",
    "Xc_test,  yc_test  = test[features],  test['y_up']\n",
    "logit_pipe = Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression(max_iter=1000))])\n",
    "tree_pipe = Pipeline([('clf', DecisionTreeClassifier(max_depth=4, random_state=42))])\n",
    "logit_pipe.fit(Xc_train, yc_train)\n",
    "tree_pipe.fit(Xc_train, yc_train)\n",
    "pred_log = logit_pipe.predict(Xc_test)\n",
    "pred_tree = tree_pipe.predict(Xc_test)\n",
    "print('Logistic Report:\\n', classification_report(yc_test, pred_log))\n",
    "print('Tree Report:\\n', classification_report(yc_test, pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(yc_test, pred_log)\n",
    "sns.heatmap(cm, annot=True, fmt='d'); plt.title('Confusion Matrix — Logistic'); plt.xlabel('Pred'); plt.ylabel('True'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5345b0f6",
   "metadata": {},
   "source": [
    "### Shuffle vs Time-Aware Split (Cautionary Demo)\n",
    "We show how shuffling can inflate scores by leaking future distributional info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD: Random split on time series (for demo purposes only)\n",
    "Xc = df_feat[features]\n",
    "yc = df_feat['y_up']\n",
    "X_tr_bad, X_te_bad, y_tr_bad, y_te_bad = train_test_split(Xc, yc, test_size=0.2, shuffle=True, random_state=42)\n",
    "logit_bad = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "logit_bad.fit(X_tr_bad, y_tr_bad)\n",
    "pred_bad = logit_bad.predict(X_te_bad)\n",
    "print('Random split (BAD) report:\\n', classification_report(y_te_bad, pred_bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d628601",
   "metadata": {},
   "source": [
    "## 6) Takeaways\n",
    "- Use returns, not raw prices, to reduce spurious results.\n",
    "- Avoid leakage with careful shifting and time-aware splits.\n",
    "- Choose metrics aligned with costs (precision/recall).\n",
    "- Pipelines improve reproducibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
