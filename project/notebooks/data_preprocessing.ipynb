{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caa9d9e6-c9c0-4fd4-9063-3de21783cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Add project root (parent of notebooks/) to Python path \n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "DATA = PROJECT_ROOT / \"data\"\n",
    "RAW = DATA / \"raw\"\n",
    "PROCESSED = DATA / \"processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aeb5c67-13b2-4938-a7d2-3ac6b4ba903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers ---\n",
    "def ts() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def save_both(df: pd.DataFrame, base_name: str):\n",
    "    pq_path = PROCESSED / f\"{base_name}.parquet\"\n",
    "    csv_path = PROCESSED / f\"{base_name}.csv\"\n",
    "    df.to_parquet(pq_path, index=False)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {pq_path.name}  |  {csv_path.name}\")\n",
    "    return pq_path, csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa54689-a756-4b80-bc9f-3d21c1e93f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: market_source-yfinance_symbol-^GSPC_name-sp500_20250820_102321.csv\n",
      "Raw shape: (251, 2)\n",
      "Saved: market_source-yfinance_symbol-^GSPC_name-sp500_20250820_102321_processed.parquet and market_source-yfinance_symbol-^GSPC_name-sp500_20250820_102321_processed.csv -> shape: (251, 2)\n",
      "\n",
      "Processing: market_source-yfinance_symbol-^VIX_name-vix_20250820_102321.csv\n",
      "Raw shape: (251, 2)\n",
      "Saved: market_source-yfinance_symbol-^VIX_name-vix_20250820_102321_processed.parquet and market_source-yfinance_symbol-^VIX_name-vix_20250820_102321_processed.csv -> shape: (251, 2)\n"
     ]
    }
   ],
   "source": [
    "# Test data preprocessing \n",
    "from src.cleaning import fill_missing_median, drop_missing, normalize_data\n",
    "\n",
    "csv_files = sorted(RAW.glob(\"*.csv\"))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(\"No CSV files found in data/raw/\")\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    print(f\"\\nProcessing: {csv_path.name}\")\n",
    "    \n",
    "    # Load raw data\n",
    "    df_raw = pd.read_csv(csv_path, parse_dates=['date'])\n",
    "    print(\"Raw shape:\", df_raw.shape)\n",
    "    \n",
    "    # Fill missing values (median for numeric)\n",
    "    df_clean = fill_missing_median(df_raw, columns=['adj_close'])\n",
    "    \n",
    "    # Drop rows with missing critical columns\n",
    "    df_clean = drop_missing(df_clean, columns=['date','adj_close'])\n",
    "    \n",
    "    # Normalize numeric columns \n",
    "    df_clean = df_clean.set_index('date')\n",
    "    df_clean[['adj_close']] = normalize_data(df_clean, columns=['adj_close'], method='minmax')\n",
    "    df_clean = df_clean.reset_index()\n",
    "\n",
    "\n",
    "    # Save processed dataset (both CSV & Parquet)\n",
    "    base_name = csv_path.stem + \"_processed\"\n",
    "    pq_path = PROCESSED / f\"{base_name}.parquet\"\n",
    "    csv_out_path = PROCESSED / f\"{base_name}.csv\"\n",
    "    \n",
    "    df_clean.to_parquet(pq_path, index=False)\n",
    "    df_clean.to_csv(csv_out_path, index=False)\n",
    "    \n",
    "    print(\"Saved:\", pq_path.name, \"and\", csv_out_path.name, \"-> shape:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a94d5cdf-1905-4e7b-9132-54dd525bb94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spx_yf_20250822_102618.csv\n",
      "Loading vix_yf_20250822_102618.csv\n",
      "Loading macro_fred_20250822_102618.csv\n",
      "SPX: (1256, 2) | VIX: (1256, 2) | Macro: (1256, 5)\n",
      "Processed dataset saved to /Users/yihanyao/bootcamp_yihan_yao/project/data/processed/preprocessed_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# ========= 2) CLEAN & PREPROCESS =========\n",
    "# Reload the raw datasets \n",
    "def load_latest_csv(prefix: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the most recent CSV file that starts with the given prefix.\"\"\"\n",
    "    files = sorted(RAW.glob(f\"{prefix}_*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files found for prefix '{prefix}' in {DATA_PROCESSED}\")\n",
    "    latest = files[-1]\n",
    "    print(f\"Loading {latest.name}\")\n",
    "    return pd.read_csv(latest, parse_dates=[\"date\"])\n",
    "\n",
    "spx = load_latest_csv(\"spx\")\n",
    "vix = load_latest_csv(\"vix\")\n",
    "macro = load_latest_csv(\"macro\")\n",
    "print(\"SPX:\", spx.shape, \"| VIX:\", vix.shape, \"| Macro:\", macro.shape)\n",
    "\n",
    "# Merge on date\n",
    "df = (\n",
    "    spx.merge(vix, on=\"date\", how=\"inner\")\n",
    "       .merge(macro, on=\"date\", how=\"left\")\n",
    "       .sort_values(\"date\")\n",
    "       .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "from src.cleaning import fill_missing_median, drop_missing, normalize_data\n",
    "\n",
    "# Core cleaning:\n",
    "# - Ensure datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "# - Drop rows missing critical fields (date, spx_close, vix)\n",
    "df = drop_missing(df, columns=[\"date\", \"spx_close\", \"vix\"])\n",
    "\n",
    "# Fill remaining numeric NaNs (e.g., macro gaps) with median to avoid losing rows\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "df = fill_missing_median(df, columns=num_cols)\n",
    "\n",
    "# Normalize some columns for ML \n",
    "df_norm = normalize_data(df.set_index(\"date\"), columns=[\"spx_close\", \"vix\"], method=\"minmax\").reset_index()\n",
    "df.update(df_norm[[\"spx_close\", \"vix\"]])\n",
    "\n",
    "# Save the processed dataset (combined SPX + VIX + Macro)\n",
    "processed_file = PROCESSED / \"preprocessed_dataset.csv\"\n",
    "df.to_csv(processed_file, index=False) \n",
    "print(f\"Processed dataset saved to {processed_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f5553-dbe4-4951-adf7-c6cfcad76439",
   "metadata": {},
   "source": [
    "### Assumptions & Rationale during Cleaning\n",
    "\n",
    "1. **Missing values filled with median**  \n",
    "   - Assumption: Stock prices (`adj_close`) are continuous and median imputation is robust to outliers.  \n",
    "   - Rationale: Preserves dataset size while reducing distortion from extreme values compared to mean imputation.  \n",
    "\n",
    "2. **Dropping rows with missing critical fields**  \n",
    "   - Assumption: `date` and `adj_close` are required for analysis; rows missing these are unusable.  \n",
    "   - Rationale: Ensures consistency in time series and prevents errors in downstream volatility modeling.  \n",
    "\n",
    "3. **Normalization (Min-Max scaling)**  \n",
    "   - Assumption: Scaling `adj_close` to [0,1] improves stability for ML models and makes features comparable.  \n",
    "   - Rationale: Prevents models from being biased by absolute price levels, focusing instead on relative changes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a963c-2941-4c1a-a8a5-ed2a711e41c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bootcamp_env]",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
